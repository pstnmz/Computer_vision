{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff33ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing breastmnist with color=False and activation=sigmoid\n",
      "Training dataset size: 499\n",
      "Calibration dataset size: 125\n",
      "Training dataset size: 499\n",
      "Calibration dataset size: 125\n"
     ]
    }
   ],
   "source": [
    "import UQ_toolbox as uq\n",
    "from medMNIST.utils import train_load_datasets_resnet as tr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gps_augment.utils.randaugment import BetterRandAugment\n",
    "\n",
    "\n",
    "def train_val_loaders(train_dataset, batch_size):\n",
    "    # Create stratified K-fold cross-validator\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Get the labels for stratification\n",
    "    labels = [label for _, label in train_dataset]\n",
    "\n",
    "    # Create a list to store the new dataloaders\n",
    "    train_loaders = []\n",
    "    val_loaders = []\n",
    "\n",
    "    for train_index, val_index in skf.split(np.zeros(len(labels)), labels):\n",
    "        train_subset = torch.utils.data.Subset(train_dataset, train_index)\n",
    "        val_subset = torch.utils.data.Subset(train_dataset, val_index)\n",
    "        \n",
    "        train_loader = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_loader = DataLoader(dataset=val_subset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        train_loaders.append(train_loader)\n",
    "        val_loaders.append(val_loader)\n",
    "    return train_loaders, val_loaders\n",
    "\n",
    "dataflag = 'breastmnist'\n",
    "color = False # True for color, False for grayscale\n",
    "activation = 'sigmoid'\n",
    "batch_size = 4000\n",
    "im_size = 224\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "size = 224  # Image size for the models\n",
    "batch_size = 4000  # Batch size for the DataLoader\n",
    "\n",
    "print(f\"Processing {dataflag} with color={color} and activation={activation}\")\n",
    "if color is True:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
    "    ])\n",
    "    \n",
    "    transform_tta = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "else:\n",
    "    # For grayscale images, repeat the single channel to make it compatible with ResNet\n",
    "    # ResNet expects 3 channels, so we repeat the single channel image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[.5], std=[.5]),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "    ])\n",
    "    \n",
    "    transform_tta = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        ])\n",
    "models = tr.load_models(dataflag, device=device)\n",
    "[train_dataset, calibration_dataset, test_dataset], [train_loader, calibration_loader, test_loader], info = tr.load_datasets(dataflag, color, size, transform, batch_size)\n",
    "train_loaders, val_loaders = train_val_loaders(train_dataset, batch_size=batch_size)\n",
    "task_type = info['task']  # Determine the task type (binary-class or multi-class)\n",
    "num_classes = len(info['label'])  # Number of classes\n",
    "[_, calibration_dataset_tta, test_dataset_tta], [_, calibration_loader_tta, test_loader_tta], _ = tr.load_datasets(dataflag, color, size, transform_tta, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBatchDimension:\n",
    "    def __call__(self, image):\n",
    "        # Ensure the image is a tensor and add batch dimension\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            return image.unsqueeze(0).float()\n",
    "        raise TypeError(\"Input should be a torch Tensor\")\n",
    "\n",
    "\n",
    "def get_prediction(model, image, device):\n",
    "    \"\"\"\n",
    "    Generates a prediction from a given model and image.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model used for prediction.\n",
    "        image (torch.Tensor): The input image tensor.\n",
    "        device (torch.device): The device to run the model on (e.g., 'cpu' or 'cuda').\n",
    "        softmax_application (bool, optional): If True, applies softmax to the prediction. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The prediction output from the model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    image = image.to(device, non_blocking=True)\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        prediction = model(image).detach().cpu()\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def extract_gps_augmentations_info(policies):\n",
    "    \"\"\"\n",
    "    Extracts N, M values and the list of policies from a list of policy filenames.\n",
    "\n",
    "    Args:\n",
    "    - policies (list of str): List of filenames in the format 'N2_M45_[(op, magnitude), (op, magnitude)].npz'.\n",
    "\n",
    "    Returns:\n",
    "    - N (int): The value of N (same for all policies).\n",
    "    - M (int): The value of M (same for all policies).\n",
    "    - formatted_policies (list of str): List of policies as strings.\n",
    "    \"\"\"\n",
    "    # Extract N and M from the first policy string\n",
    "    if not policies:\n",
    "        return None, None, []\n",
    "\n",
    "    first_policy = policies[0]\n",
    "    match = re.search(r'N(\\d+)_M(\\d+)', first_policy)\n",
    "    if match:\n",
    "        N = int(match.group(1))\n",
    "        M = int(match.group(2))\n",
    "    \n",
    "    formatted_policies = []\n",
    "\n",
    "    # Extract the policy part from each filename and keep it as a string\n",
    "    for policy in policies:\n",
    "        policy_match = re.search(r'\\[(.*?)\\]', policy)\n",
    "        if policy_match:\n",
    "            policy_str = f\"[{policy_match.group(1)}]\"  # Add brackets back around the tuple\n",
    "            formatted_policies.append(policy_str)\n",
    "\n",
    "    return N, M, formatted_policies\n",
    "\n",
    "\n",
    "def to_3_channels(img):\n",
    "    if img.mode == 'L':  # Grayscale image\n",
    "        img = img.convert('RGB')  # Convert to 3 channels by duplicating\n",
    "    return img\n",
    "\n",
    "def to_1_channel(img):\n",
    "    img = img.convert('L')  # Convert back to grayscale\n",
    "    return img\n",
    "\n",
    "\n",
    "def apply_augmentations(dataset, nb_augmentations, usingBetterRandAugment, n, m, image_normalization, nb_channels, mean, std, image_size, transformations=False, batch_size=None):\n",
    "    \"\"\"\n",
    "    Apply augmentations to the images.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): Batch of images.\n",
    "        transformations (callable or list): Transformations to apply to each image.\n",
    "        nb_augmentations (int): Number of augmentations to apply per image.\n",
    "        usingBetterRandAugment (bool): If True, use BetterRandAugment with provided policies.\n",
    "        n (int): Number of augmentation transformations to apply when using BetterRandAugment.\n",
    "        m (int): Magnitude of the augmentation transformations when using BetterRandAugment.\n",
    "        batch_norm (bool): Whether to use batch normalization.\n",
    "        nb_channels (int): Number of channels in the input images.\n",
    "        mean (list or None): Mean for normalization.\n",
    "        std (list or None): Standard deviation for normalization.\n",
    "        image_size (int): Size of the input images.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Augmented images.\n",
    "    \"\"\"\n",
    "    augmented_inputs = []\n",
    "    if usingBetterRandAugment:\n",
    "        if isinstance(transformations, list):\n",
    "            rand_aug_policies = [BetterRandAugment(n=n, m=m, resample=False, transform=policy, verbose=True, randomize_sign=False, image_size=image_size) for policy in transformations]\n",
    "            \n",
    "        elif transformations is False:\n",
    "            rand_aug_policies = [BetterRandAugment(n, m, True, False, randomize_sign=False, image_size=image_size) for _ in range(nb_augmentations)] \n",
    "\n",
    "        augmentations = [transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Lambda(lambda img: img.convert(\"RGB\")),  # Ensure image is in RGB format\n",
    "                    *([to_3_channels] if nb_channels == 1 else []),  # Conditionally add to_3_channels\n",
    "                    rand_aug,\n",
    "                    *([to_1_channel] if nb_channels == 1 else []),  # Conditionally add to_1_channel\n",
    "                    transforms.PILToTensor(),\n",
    "                    transforms.Lambda(lambda x: x.float()) if nb_channels == 1 else transforms.ConvertImageDtype(torch.float),\n",
    "                    *([transforms.Normalize(mean=mean, std=std)] if image_normalization else [])\n",
    "                ]) for rand_aug in rand_aug_policies]\n",
    "        \n",
    "        for i, augmentation in enumerate(augmentations):\n",
    "            print(f\"Applying augmentation n : {i}\")\n",
    "\n",
    "            for subds in dataset.dataset.datasets:\n",
    "                subds.transform = transforms.Compose([transforms.ToTensor()])\n",
    "            # Get first batch of original images\n",
    "            #dataset.dataset.datasets.transform = transforms.Compose([transforms.ToTensor()])\n",
    "            data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "            batch = next(iter(data_loader))\n",
    "            original_images = batch[0]\n",
    "\n",
    "            for subds in dataset.dataset.datasets:\n",
    "                subds.transform = augmentation\n",
    "            # Get first batch of augmented images\n",
    "            #dataset.dataset.datasets.transform = augmentation\n",
    "            print(augmentation.transforms[3].ops)\n",
    "            data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "            batch = next(iter(data_loader))\n",
    "            augmented_images = batch[0]\n",
    "\n",
    "            # Display side by side\n",
    "            print(\"Original and Augmented images (side by side):\")\n",
    "            fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8, 12))\n",
    "            for idx in range(min(3, original_images.shape[0])):\n",
    "                # Original\n",
    "                img_orig = original_images[idx]\n",
    "                img_orig_np = img_orig.cpu().numpy()\n",
    "                if img_orig_np.shape[0] == 1:\n",
    "                    img_orig_np = img_orig_np.squeeze(0)\n",
    "                    axes[idx, 0].imshow(img_orig_np, cmap='gray')\n",
    "                else:\n",
    "                    img_orig_np = np.transpose(img_orig_np, (1, 2, 0))\n",
    "                    axes[idx, 0].imshow(img_orig_np)\n",
    "                axes[idx, 0].set_title(f\"Original Image {idx+1}\")\n",
    "                axes[idx, 0].axis('off')\n",
    "\n",
    "                # Augmented\n",
    "                img_aug = augmented_images[idx]\n",
    "                img_aug_np = img_aug.cpu().numpy()\n",
    "                if img_aug_np.shape[0] == 1:\n",
    "                    img_aug_np = img_aug_np.squeeze(0)\n",
    "                    axes[idx, 1].imshow(img_aug_np, cmap='gray')\n",
    "                else:\n",
    "                    img_aug_np = np.transpose(img_aug_np, (1, 2, 0))\n",
    "                    axes[idx, 1].imshow(img_aug_np)\n",
    "                axes[idx, 1].set_title(f\"Augmented Image {idx+1}\")\n",
    "                axes[idx, 1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "                \n",
    "\n",
    "def get_batch_predictions(models, augmented_inputs, device):\n",
    "    \"\"\"\n",
    "    Get predictions for the augmented inputs.\n",
    "\n",
    "    Args:\n",
    "        models (torch.nn.Module or list): Model or list of models to use for predictions.\n",
    "        augmented_inputs (torch.Tensor): Augmented images.\n",
    "        device (torch.device): Device to run the models on.\n",
    "        softmax_application (bool): If True, applies softmax to the prediction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Batch predictions.\n",
    "    \"\"\"\n",
    "    if isinstance(models, list):\n",
    "        batch_predictions = []\n",
    "        for model in models:\n",
    "            prediction = get_prediction(model, augmented_inputs, device)\n",
    "            batch_predictions.append(prediction)\n",
    "    else:\n",
    "        prediction = get_prediction(models, augmented_inputs, device)\n",
    "        batch_predictions = [prediction]\n",
    "    \n",
    "    batch_predictions = torch.stack(batch_predictions, dim=0)  # Shape: [num_models, batch_size * num_augmentations, num_classes]\n",
    "    return batch_predictions\n",
    "\n",
    "\n",
    "def average_predictions(batch_predictions, output_activation=None):\n",
    "    \"\"\"\n",
    "    Average predictions across models and group augmentations back with their respective images.\n",
    "\n",
    "    Args:\n",
    "        batch_predictions (torch.Tensor): Batch predictions. Shape: [num_models, batch_size * num_augmentations, num_classes].\n",
    "        output_activation (str, optional): Activation function to apply to the predictions. \n",
    "                                           Options: 'softmax', 'sigmoid', or None. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Averaged predictions. Shape: [batch_size * num_augmentations, num_classes].\n",
    "    \"\"\"\n",
    "    # Average predictions across models\n",
    "    averaged_predictions = torch.mean(batch_predictions, dim=0)  # Shape: [batch_size * num_augmentations, num_classes]\n",
    "\n",
    "    # Apply the specified activation function\n",
    "    if output_activation == 'softmax':\n",
    "        averaged_predictions = torch.nn.functional.softmax(averaged_predictions, dim=-1)\n",
    "    elif output_activation == 'sigmoid':\n",
    "        averaged_predictions = torch.sigmoid(averaged_predictions)\n",
    "\n",
    "    return averaged_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2349989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_randaugment_and_store_results(\n",
    "    dataset, models, N, M, num_policies, device, folder_name='savedpolicies',\n",
    "    image_normalization=False, mean=False, std=False, nb_channels=1, image_size=51,\n",
    "    output_activation=None, batch_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply RandAugment transformations to the data and store the results, one augmentation at a time.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    for i in range(num_policies):\n",
    "    \n",
    "        print(f\"Applying augmentation policy {i+1}/{num_policies}\")\n",
    "        # Apply augmentation and get augmented images\n",
    "        apply_augmentations(\n",
    "            dataset, 1, True, N, M, image_normalization, nb_channels, mean, std, image_size, batch_size=batch_size\n",
    "        )\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "297c8ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying augmentation policy 1/500\n",
      "Applying augmentation n : 0\n",
      "[(<function SolarizeAdd at 0x727acbbb1ee0>, 256, 128), (<function Cutout at 0x727acbbb22a0>, 0, 0.5)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "color must be int or single-element tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mapply_randaugment_and_store_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/data/psteinmetz/archive_notebooks/Documents/medMNIST/gps_augment/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mim_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mim_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataflag\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_calibration_set\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_normalization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mapply_randaugment_and_store_results\u001b[0;34m(dataset, models, N, M, num_policies, device, folder_name, image_normalization, mean, std, nb_channels, image_size, output_activation, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying augmentation policy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_policies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply augmentation and get augmented images\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mapply_augmentations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_normalization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m, in \u001b[0;36mapply_augmentations\u001b[0;34m(dataset, nb_augmentations, usingBetterRandAugment, n, m, image_normalization, nb_channels, mean, std, image_size, transformations, batch_size)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(augmentation\u001b[38;5;241m.\u001b[39mtransforms[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mops)\n\u001b[1;32m    130\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mdataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 131\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m augmented_images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Display side by side\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/medmnist/dataset.py:144\u001b[0m, in \u001b[0;36mMedMNIST2D.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    141\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/mnt/data/psteinmetz/archive_notebooks/Documents/gps_augment/utils/randaugment.py:251\u001b[0m, in \u001b[0;36mBetterRandAugment.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    249\u001b[0m         m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(m)\n\u001b[1;32m    250\u001b[0m     val \u001b[38;5;241m=\u001b[39m (m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_magnitude) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(maxval \u001b[38;5;241m-\u001b[39m minval) \u001b[38;5;241m+\u001b[39m minval\n\u001b[0;32m--> 251\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/mnt/data/psteinmetz/archive_notebooks/Documents/gps_augment/utils/randaugment.py:125\u001b[0m, in \u001b[0;36mCutout\u001b[0;34m(img, v, fcolor)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[1;32m    124\u001b[0m v \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m*\u001b[39m img\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCutoutAbs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcolor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/psteinmetz/archive_notebooks/Documents/gps_augment/utils/randaugment.py:147\u001b[0m, in \u001b[0;36mCutoutAbs\u001b[0;34m(img, v, fcolor)\u001b[0m\n\u001b[1;32m    145\u001b[0m     color \u001b[38;5;241m=\u001b[39m fcolor\n\u001b[1;32m    146\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 147\u001b[0m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageDraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrectangle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/PIL/ImageDraw.py:409\u001b[0m, in \u001b[0;36mImageDraw.rectangle\u001b[0;34m(self, xy, fill, outline, width)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrectangle\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     xy: Coords,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m     width: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Draw a rectangle.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     ink, fill_ink \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getink\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fill_ink \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mdraw_rectangle(xy, fill_ink, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/venv_medMNIST/lib/python3.12/site-packages/PIL/ImageDraw.py:170\u001b[0m, in \u001b[0;36mImageDraw._getink\u001b[0;34m(self, ink, fill)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fill, numbers\u001b[38;5;241m.\u001b[39mNumber):\n\u001b[1;32m    169\u001b[0m             fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(fill, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_image)\n\u001b[0;32m--> 170\u001b[0m         result_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_ink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_ink, result_fill\n",
      "\u001b[0;31mTypeError\u001b[0m: color must be int or single-element tuple"
     ]
    }
   ],
   "source": [
    "apply_randaugment_and_store_results(calibration_dataset, models, 2, 45, 500, device, folder_name=f'/mnt/data/psteinmetz/archive_notebooks/Documents/medMNIST/gps_augment/{im_size}*{im_size}/{dataflag}_calibration_set', image_normalization=True, mean=[.5], std=[.5], image_size=im_size, nb_channels=3, output_activation=activation, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbca8f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calibration_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db57d77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset BreastMNIST of size 224 (breastmnist_224)\n",
       "     Number of datapoints: 546\n",
       "     Root location: /home/psteinmetz/.medmnist\n",
       "     Split: train\n",
       "     Task: binary-class\n",
       "     Number of channels: 1\n",
       "     Meaning of labels: {'0': 'malignant', '1': 'normal, benign'}\n",
       "     Number of samples: {'train': 546, 'val': 78, 'test': 156}\n",
       "     Description: The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.\n",
       "     License: CC BY 4.0,\n",
       " Dataset BreastMNIST of size 224 (breastmnist_224)\n",
       "     Number of datapoints: 78\n",
       "     Root location: /home/psteinmetz/.medmnist\n",
       "     Split: val\n",
       "     Task: binary-class\n",
       "     Number of channels: 1\n",
       "     Meaning of labels: {'0': 'malignant', '1': 'normal, benign'}\n",
       "     Number of samples: {'train': 546, 'val': 78, 'test': 156}\n",
       "     Description: The BreastMNIST is based on a dataset of 780 breast ultrasound images. It is categorized into 3 classes: normal, benign, and malignant. As we use low-resolution images, we simplify the task into binary classification by combining normal and benign as positive and classifying them against malignant as negative. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images of 1×500×500 are resized into 1×28×28.\n",
       "     License: CC BY 4.0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calibration_dataset.dataset.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ab813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_medMNIST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
