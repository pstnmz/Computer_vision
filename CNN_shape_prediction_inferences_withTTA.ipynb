{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, recall_score, accuracy_score\n",
    "from skimage.measure import EllipseModel, find_contours\n",
    "from skimage.draw import ellipse\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import ndimage, spatial, special\n",
    "import scipy.optimize as optimize\n",
    "import os\n",
    "from radiomics import featureextractor\n",
    "import radiomics\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datatable as dt\n",
    "from skimage import io, transform\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "from PIL import Image\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs available:\", num_gpus)\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "except Exception as e:\n",
    "    print(\"Error while checking GPUs:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your CNN model\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout_conv = nn.Dropout(0.3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32*12*12, 64)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x= self.dropout_conv(x)\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.dropout_conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, shape = sample['image'], sample['shape']\n",
    "        if shape=='Round':\n",
    "            shape=0\n",
    "        elif shape=='Irregular':\n",
    "            shape=1\n",
    "        elif shape=='Ambiguous':\n",
    "            shape=2\n",
    "        return {'image': torch.from_numpy(image).unsqueeze(0),\n",
    "                'shape': torch.from_numpy(np.asarray(shape))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, sample):\n",
    "        image, shape = sample['image'], sample['shape']\n",
    "        norm = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        return {'image': norm(image.float()),\n",
    "                'shape': shape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialCutsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_shape, transform=None, for_trainning=False, mean=False, std=False, downsample=False):\n",
    "        self.data = data_shape\n",
    "        self.transform=transform\n",
    "        self.for_trainning = for_trainning\n",
    "        self.mean= mean\n",
    "        self.std = std\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            df_majority = self.data[self.data.iloc[:, 1]=='Irregular']\n",
    "            df_majority_downsampled = resample(df_majority, replace=False, n_samples=1200, random_state=125)\n",
    "            self.data = pd.concat((self.data[self.data['Shape'] != 'Irregular'], df_majority_downsampled))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.iloc[:, 0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = self.data.iloc[idx, 0]\n",
    "        image = io.imread(img_name)\n",
    "        shape = self.data.iloc[idx, 1]\n",
    "        sample = {'image': image, 'shape': shape, 'name': img_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for evaluation metrics\n",
    "def accuracy(outputs, labels):\n",
    "    preds = outputs > 0.5 \n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "def f1(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return f1_score(labels, preds, average='binary')\n",
    "\n",
    "def calculate_sensitivity(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return recall_score(labels, preds, average='binary')\n",
    "\n",
    "def roc_auc(outputs, labels):\n",
    "    probs = outputs\n",
    "    return roc_auc_score(labels, probs)\n",
    "\n",
    "def compute_confusion_matrix(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return confusion_matrix(labels, preds)\n",
    "\n",
    "def calculate_specificity(cm):\n",
    "    TN = cm[0, 0]  # True negatives\n",
    "    FP = cm[0, 1]  # False positives\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "def display_confusion_matrix(cm):\n",
    "    # Define class names\n",
    "    class_names = ['Round', 'Irregular']\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_val_metrics(all_preds, all_labels):\n",
    "\n",
    "    # Concatenate predictions and labels\n",
    "    # all_preds = torch.from_numpy(np.concatenate(all_preds, axis=0))\n",
    "    # all_labels = torch.from_numpy(np.concatenate(all_labels, axis=0))\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = accuracy(all_preds, all_labels)\n",
    "    f1_result = f1(all_preds, all_labels)\n",
    "    roc_auc_result = roc_auc(all_preds, all_labels)\n",
    "    cm = compute_confusion_matrix(all_preds, all_labels)\n",
    "    display_confusion_matrix(cm)\n",
    "    sensitivity_value = calculate_sensitivity(all_preds, all_labels)\n",
    "    specificity_value = calculate_specificity(cm)\n",
    "\n",
    "    print('Accuracy: {:.6f} \\tF1 Score: {:.6f} \\tROC AUC: {:.6f} \\tSpecificity: {:.6f} \\tSensitivity: {:.6f}'.format(\n",
    "        acc, f1_result, roc_auc_result, specificity_value, sensitivity_value))\n",
    "    \n",
    "# Function to make predictions\n",
    "def predict(models, image):\n",
    "    image = image.to(device)\n",
    "    predictions = [model(image).cpu().detach().numpy() for model in models]\n",
    "    return predictions\n",
    "\n",
    "def plot_calibration_curve(y_true, y_prob):\n",
    "    # Calculate the calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10, strategy='uniform')\n",
    "    \n",
    "    return prob_true, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model_paths = ['model_shape_0_augmented.pt', 'model_shape_1_augmented.pt', 'model_shape_2_augmented.pt', 'model_shape_3_augmented.pt', 'model_shape_4_augmented.pt']\n",
    "models = []\n",
    "\n",
    "for path in model_paths:\n",
    "    model = simpleNet()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transform = transforms.Compose([\n",
    "    transforms.RandAugment(),  # Apply RandAugment\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean = 87.42158495776914\n",
    "std = 29.82248099334633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 87.42158495776914\n",
    "std = 29.82248099334633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = '/mnt/data/psteinmetz/neotex/data_CNN/images_15062024/evaluation_IRM_villes/'\n",
    "data = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(glob.glob(f'{images_path}*/*png')),\n",
    "        pd.DataFrame(\n",
    "            [x.split('/')[-1][:-4] for x in glob.glob(f'{images_path}*/*png')]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            [k.split('/')[-2] for k in glob.glob(f'{images_path}*/*png')]\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data.columns = ['Path', 'ID', 'Shape']\n",
    "data.set_index('ID', inplace=True)\n",
    "\n",
    "\n",
    "axialcuts_dataset_eval = AxialCutsDataset(data_shape=data, downsample=False)\n",
    "data_without_amb = axialcuts_dataset_eval.data[axialcuts_dataset_eval.data['Shape']!='Ambiguous']\n",
    "data_amb = axialcuts_dataset_eval.data[axialcuts_dataset_eval.data['Shape']=='Ambiguous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path_tr = '/mnt/data/psteinmetz/neotex/data_CNN/images_15062024/'\n",
    "data_tr = pd.concat(\n",
    "    (\n",
    "        pd.DataFrame(glob.glob(f'{images_path_tr}*/*png')),\n",
    "        pd.DataFrame(\n",
    "            [x.split('/')[-1][:-4] for x in glob.glob(f'{images_path_tr}*/*png')]\n",
    "        ),\n",
    "        pd.DataFrame(\n",
    "            [k.split('/')[-2] for k in glob.glob(f'{images_path_tr}*/*png')]\n",
    "        ),\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "data_tr.columns = ['Path', 'ID', 'Shape']\n",
    "data_tr.set_index('ID', inplace=True)\n",
    "axialcuts_dataset_tr = AxialCutsDataset(data_shape=data_tr, downsample=False)\n",
    "data_amb_tr = axialcuts_dataset_tr.data[axialcuts_dataset_tr.data['Shape']=='Ambiguous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_amb = AxialCutsDataset(data_shape=data_without_amb, downsample=False, transform=data_transforms)\n",
    "data_amb = AxialCutsDataset(data_shape=data_amb, downsample=False, transform=data_transforms)\n",
    "data_amb_tr = AxialCutsDataset(data_shape=data_amb_tr, downsample=False, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = DataLoader(data_without_amb, num_workers=12)\n",
    "eval_data_amb = DataLoader(data_amb, num_workers=12)\n",
    "tr_data_amb = DataLoader(data_amb_tr, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results = []\n",
    "mean_pred = []\n",
    "true_labels = []\n",
    "models = [model.to(device) for model in models]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred.append(np.mean(pred_probs))\n",
    "        true_labels.append(labels.item())\n",
    "        all_results.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pred = [[all_results[k]['predicted_probabilities'][i].ravel() for k in range(len(all_results))] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_val_metrics(np.array(mean_pred), np.array(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "i = 0\n",
    "for res in models_pred:\n",
    "    prob_true, prob_pred = plot_calibration_curve(np.array(true_labels), np.array(res).ravel())\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f'Model_{i} Calibration Curve', alpha = 0.1)\n",
    "    i=i+1\n",
    "prob_true, prob_pred = plot_calibration_curve(np.array(true_labels), np.array(mean_pred))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=f'Mean Calibration Curve')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results_amb = []\n",
    "mean_pred_amb = []\n",
    "true_labels_amb = []\n",
    "models = [model.to(device) for model in models]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data_amb:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']  # Assuming 'shape' is the label\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred_amb.append(np.mean(pred_probs))\n",
    "        true_labels_amb.append(labels.item())\n",
    "        all_results_amb.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results_amb_tr = []\n",
    "mean_pred_amb_tr = []\n",
    "true_labels_amb_tr = []\n",
    "models = [model.to(device) for model in models]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in tr_data_amb:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']  # Assuming 'shape' is the label\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred_amb_tr.append(np.mean(pred_probs))\n",
    "        true_labels_amb_tr.append(labels.item())\n",
    "        all_results_amb_tr.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_results = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class']]\n",
    "bad_results = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_results_round = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class'] and all_results[k]['true_label'] == 0]\n",
    "good_results_irregular = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class'] and all_results[k]['true_label'] == 1]\n",
    "\n",
    "bad_results_round = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class'] and all_results[k]['true_label'] == 0]\n",
    "bad_results_irregular = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class'] and all_results[k]['true_label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_prob_round = [good_results_round[k]['mean'] for k in range(len(good_results_round))]\n",
    "bad_prob_round = [bad_results_round[k]['mean'] for k in range(len(bad_results_round))]\n",
    "good_prob_irregular = [good_results_irregular[k]['mean'] for k in range(len(good_results_irregular))]\n",
    "bad_prob_irregular = [bad_results_irregular[k]['mean'] for k in range(len(bad_results_irregular))]\n",
    "amb_means = [all_results_amb[k]['mean'] for k in range(len(all_results_amb))]\n",
    "amb_tr_means = [all_results_amb_tr[k]['mean'] for k in range(len(all_results_amb_tr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data in a long-form DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Prob': good_prob_round + bad_prob_irregular + bad_prob_round + good_prob_irregular + amb_means + amb_tr_means,\n",
    "    'Category': ['Correct Results Round'] * len(good_prob_round) + ['Incorrect Results Irregular'] * len(bad_prob_irregular) + ['Incorrect Results Round'] * len(bad_prob_round) + ['Correct Results Irregular'] * len(good_prob_irregular) + ['Results Ambigueous Eval DB'] * len(amb_means) + ['Results Ambigueous Train DB'] * len(amb_tr_means)\n",
    "})\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the violin plot\n",
    "sns.violinplot(x='Category', y='Prob', data=df, inner=None, palette='muted')\n",
    "\n",
    "# Add the scatter points\n",
    "sns.swarmplot(x='Category', y='Prob', data=df, color='k', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Predicted probabilities deep ensembles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Category', y='Prob', data=df, palette='muted')\n",
    "sns.swarmplot(x='Category', y='Prob', data=df, color='k', alpha=0.3)\n",
    "# Show the plot\n",
    "plt.title('Predicted probabilities deep ensembles')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stds = [good_results[k]['std'] for k in range(len(good_results))]\n",
    "bad_stds = [bad_results[k]['std'] for k in range(len(bad_results))]\n",
    "amb_stds = [all_results_amb[k]['std'] for k in range(len(all_results_amb))]\n",
    "amb_tr_stds = [all_results_amb_tr[k]['std'] for k in range(len(all_results_amb_tr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data in a long-form DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Std': good_stds + bad_stds + amb_stds + amb_tr_stds,\n",
    "    'Category': ['Correct Results'] * len(good_stds) + ['Incorrect Results'] * len(bad_stds) + ['Ambiguous Results evaluation dataset'] * len(amb_stds) + ['Ambiguous Results train dataset'] * len(amb_tr_stds)\n",
    "})\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the violin plot\n",
    "sns.violinplot(x='Category', y='Std', data=df, inner=None, palette='muted')\n",
    "\n",
    "# Add the scatter points\n",
    "sns.swarmplot(x='Category', y='Std', data=df, color='k', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Standard deviations deep ensembles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Category', y='Std', data=df, palette='muted')\n",
    "sns.swarmplot(x='Category', y='Std', data=df, color='k', alpha=0.3)\n",
    "# Show the plot\n",
    "plt.title('Standard deviations deep ensembles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models\n",
    "model_paths = ['model_shape_0_augmented.pt', 'model_shape_1_augmented.pt', 'model_shape_2_augmented.pt', 'model_shape_3_augmented.pt', 'model_shape_4_augmented.pt']\n",
    "models = []\n",
    "\n",
    "for path in model_paths:\n",
    "    model = simpleNet()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results = []\n",
    "mean_pred = []\n",
    "true_labels = []\n",
    "models = [model.to(device) for model in models]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred.append(np.mean(pred_probs))\n",
    "        true_labels.append(labels.item())\n",
    "        all_results.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pred = [[all_results[k]['predicted_probabilities'][i].ravel() for k in range(len(all_results))] for i in range(5)]\n",
    "calculate_val_metrics(np.array(mean_pred), np.array(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n",
    "i = 0\n",
    "for res in models_pred:\n",
    "    prob_true, prob_pred = plot_calibration_curve(np.array(true_labels), np.array(res).ravel())\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f'Model_{i} Calibration Curve', alpha = 0.1)\n",
    "    i=i+1\n",
    "prob_true, prob_pred = plot_calibration_curve(np.array(true_labels), np.array(mean_pred))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label=f'Mean Calibration Curve')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results_amb = []\n",
    "mean_pred_amb = []\n",
    "true_labels_amb = []\n",
    "models = [model.to(device) for model in models]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in eval_data_amb:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']  # Assuming 'shape' is the label\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred_amb.append(np.mean(pred_probs))\n",
    "        true_labels_amb.append(labels.item())\n",
    "        all_results_amb.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "all_results_amb_tr = []\n",
    "mean_pred_amb_tr = []\n",
    "true_labels_amb_tr = []\n",
    "models = [model.to(device) for model in models]\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    for batch in tr_data_amb:\n",
    "        images = batch['image']\n",
    "        labels = batch['shape']  # Assuming 'shape' is the label\n",
    "\n",
    "        pred_probs = predict(models, images)\n",
    "        \n",
    "        # Collect the results\n",
    "        mean_pred_amb_tr.append(np.mean(pred_probs))\n",
    "        true_labels_amb_tr.append(labels.item())\n",
    "        all_results_amb_tr.append({\n",
    "            'true_label': labels.item(),\n",
    "            'predicted_probabilities': pred_probs,\n",
    "            'predicted_class': int(np.mean(pred_probs) > 0.5),\n",
    "            'std': np.std(pred_probs),\n",
    "            'mean': np.mean(pred_probs)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_results = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class']]\n",
    "bad_results = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class']]\n",
    "\n",
    "good_results_round = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class'] and all_results[k]['true_label'] == 0]\n",
    "good_results_irregular = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] == all_results[k]['predicted_class'] and all_results[k]['true_label'] == 1]\n",
    "\n",
    "bad_results_round = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class'] and all_results[k]['true_label'] == 0]\n",
    "bad_results_irregular = [all_results[k] for k in range(len(all_results)) if all_results[k]['true_label'] != all_results[k]['predicted_class'] and all_results[k]['true_label'] == 1]\n",
    "\n",
    "good_prob_round = [good_results_round[k]['mean'] for k in range(len(good_results_round))]\n",
    "bad_prob_round = [bad_results_round[k]['mean'] for k in range(len(bad_results_round))]\n",
    "good_prob_irregular = [good_results_irregular[k]['mean'] for k in range(len(good_results_irregular))]\n",
    "bad_prob_irregular = [bad_results_irregular[k]['mean'] for k in range(len(bad_results_irregular))]\n",
    "amb_means = [all_results_amb[k]['mean'] for k in range(len(all_results_amb))]\n",
    "amb_tr_means = [all_results_amb_tr[k]['mean'] for k in range(len(all_results_amb_tr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data in a long-form DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Prob': good_prob_round + bad_prob_irregular + bad_prob_round + good_prob_irregular + amb_means + amb_tr_means,\n",
    "    'Category': ['Correct Results Round'] * len(good_prob_round) + ['Incorrect Results Irregular'] * len(bad_prob_irregular) + ['Incorrect Results Round'] * len(bad_prob_round) + ['Correct Results Irregular'] * len(good_prob_irregular) + ['Results Ambigueous Eval DB'] * len(amb_means) + ['Results Ambigueous Train DB'] * len(amb_tr_means)\n",
    "})\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the violin plot\n",
    "sns.violinplot(x='Category', y='Prob', data=df, inner=None, palette='muted')\n",
    "\n",
    "# Add the scatter points\n",
    "sns.swarmplot(x='Category', y='Prob', data=df, color='k', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Predicted probabilities deep ensembles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Category', y='Prob', data=df, palette='muted')\n",
    "sns.swarmplot(x='Category', y='Prob', data=df, color='k', alpha=0.3)\n",
    "# Show the plot\n",
    "plt.title('Predicted probabilities deep ensembles')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stds = [good_results[k]['std'] for k in range(len(good_results))]\n",
    "bad_stds = [bad_results[k]['std'] for k in range(len(bad_results))]\n",
    "amb_stds = [all_results_amb[k]['std'] for k in range(len(all_results_amb))]\n",
    "amb_tr_stds = [all_results_amb_tr[k]['std'] for k in range(len(all_results_amb_tr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data in a long-form DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Std': good_stds + bad_stds + amb_stds + amb_tr_stds,\n",
    "    'Category': ['Correct Results'] * len(good_stds) + ['Incorrect Results'] * len(bad_stds) + ['Ambiguous Results evaluation dataset'] * len(amb_stds) + ['Ambiguous Results train dataset'] * len(amb_tr_stds)\n",
    "})\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the violin plot\n",
    "sns.violinplot(x='Category', y='Std', data=df, inner=None, palette='muted')\n",
    "\n",
    "# Add the scatter points\n",
    "sns.swarmplot(x='Category', y='Std', data=df, color='k', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Standard deviations deep ensembles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create the boxplot\n",
    "sns.boxplot(x='Category', y='Std', data=df, palette='muted')\n",
    "sns.swarmplot(x='Category', y='Std', data=df, color='k', alpha=0.3)\n",
    "# Show the plot\n",
    "plt.title('Standard deviations deep ensembles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
